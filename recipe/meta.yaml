# if you wish to build release candidate number X, append the version string with ".rcX"
{% set version = "2.7.0" %}
{% set build = 0 %}

# Use a higher build number for the CUDA variant, to ensure that it's
# preferred by conda's solver, and it's preferentially
# installed where the platform supports it.
{% if cuda_compiler_version != "None" %}
{% set build = build + 200 %}
{% endif %}

{% if blas_impl == "mkl" %}
{% set build = build + 100 %}
{% endif %}

# see https://github.com/pytorch/pytorch/blame/v{{ version }}/.ci/docker/ci_commit_pins/triton.txt
# pytorch and triton are released in tandem, see notes in their release process
# https://github.com/pytorch/pytorch/blob/main/RELEASE.md#triton-dependency-for-the-release
{% set triton = "3.3.0" %}

# TODO Temporary pin, remove me
{% set mkl = "<2025" %}

package:
  name: libtorch
  version: {{ version.replace("-", ".") }}

source:
{% if "rc" in version %}
  # - git_url: https://github.com/pytorch/pytorch.git
  #   git_rev: v{{ version.replace(".rc", "-rc") }}
  # we cannot apply patches to submodules when checking out with git_url, because
  # then conda switches the patch-application to use git, which cannot construct
  # a usable ancestor from outside the submodule; the only option then is to
  # pull in the submodules separately.
  - url: https://github.com/pytorch/pytorch/archive/refs/tags/v{{ version }}.tar.gz
    sha256: 04ae0a8babdc9cb9dfc4f8746b2b8aa0f8ed0f9e92835cc4af0bcb01e3969e51
{% else %}
  # The "pytorch-v" tarballs contain submodules; the "pytorch-" ones don't.
  - url: https://github.com/pytorch/pytorch/releases/download/v{{ version }}/pytorch-v{{ version }}.tar.gz
    sha256: ecca266fa2de4235a9fd5a18a33299a9de55ab3babb87f8c297c1c9ab8d436bd
{% endif %}
    patches:
      - patches/0001-Force-usage-of-python-3-and-error-without-numpy.patch
      # backport https://github.com/pytorch/pytorch/pull/137084
      - patches/0002-Help-find-numpy.patch
      - patches/0003-Fix-duplicate-linker-script.patch  # [cuda_compiler_version != "None" and aarch64]
      # conda-specific patch, lets us override CUDA paths
      - patches/0004-Allow-overriding-CUDA-related-paths.patch
      # fix BLAS calling convention for openblas
      - patches/0005-Use-BLAS_USE_CBLAS_DOT-for-OpenBLAS-builds.patch
      # fix mkl-2024 issue
      # https://github.com/pytorch/pytorch/pull/143894
      - patches/0006-fix-issue-142484.patch
      - patches/0007-Fix-FindOpenBLAS.patch
      # point to headers that are now living in $PREFIX/include instead of $SP_DIR/torch/include
      - patches/0008-point-include-paths-to-PREFIX-include.patch
      - patches/0009-Add-conda-prefix-to-inductor-include-paths.patch
      - patches/0010-make-ATEN_INCLUDE_DIR-relative-to-TORCH_INSTALL_PREF.patch
      - patches/0011-remove-DESTINATION-lib-from-CMake-install-TARGETS-di.patch                       # [win]
      - patches/0012-avoid-deprecated-find_package-CUDA-in-caffe2-CMake-m.patch
      # backport https://github.com/pytorch/pytorch/pull/148668
      - patches/0013-Fix-CUPTI-lookup-to-include-target-directory.patch
      # backport (practically speaking) https://github.com/pytorch/pytorch/pull/149861
      - patches/0014-Always-use-system-nvtx.patch

  # submodules; not included in default tarball due to https://github.com/dear-github/dear-github/issues/214
  # and pytorch doesn't provide them for rc's yet: https://github.com/pytorch/pytorch/issues/150649
  # see https://github.com/pytorch/pytorch/tree/{{ version }}/third_party
  - folder: third_party/FP16
    git_url: https://github.com/Maratyszcza/FP16.git
    git_rev: 4dfe081cf6bcd15db339cf2680b9281b8451eeb3
  - folder: third_party/FXdiv
    git_url: https://github.com/Maratyszcza/FXdiv.git
    git_rev: b408327ac2a15ec3e43352421954f5b1967701d1
  - folder: third_party/NNPACK
    git_url: https://github.com/Maratyszcza/NNPACK.git
    git_rev: c07e3a0400713d546e0dea2d5466dd22ea389c73
  # - folder: third_party/NVTX  # skipped due to USE_SYSTEM_NVTX
  - folder: third_party/VulkanMemoryAllocator
    git_url: https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git
    git_rev: a6bfc237255a6bac1513f7c1ebde6d8aed6b5191
  - folder: third_party/XNNPACK
    git_url: https://github.com/google/XNNPACK.git
    git_rev: 51a0103656eff6fc9bfd39a4597923c4b542c883
  - folder: third_party/benchmark
    git_url: https://github.com/google/benchmark.git
    git_rev: 0d98dba29d66e93259db7daa53a9327df767a415
  - folder: third_party/composable_kernel
    git_url: https://github.com/ROCm/composable_kernel.git
    git_rev: 8086bbe3a78d931eb96fe12fdc014082e18d18d3
  - folder: third_party/cpp-httplib
    git_url: https://github.com/yhirose/cpp-httplib.git
    git_rev: 3b6597bba913d51161383657829b7e644e59c006
  - folder: third_party/cpuinfo
    git_url: https://github.com/pytorch/cpuinfo.git
    git_rev: 1e83a2fdd3102f65c6f1fb602c1b320486218a99
  - folder: third_party/cudnn_frontend
    git_url: https://github.com/NVIDIA/cudnn-frontend.git
    git_rev: 91b7532f3386768bba4f444ee7672b497f34da8a
  - folder: third_party/cutlass
    git_url: https://github.com/NVIDIA/cutlass.git
    git_rev: afa1772203677c5118fcd82537a9c8fefbcc7008
  # - folder: third_party/eigen  # skipped due to USE_SYSTEM_EIGEN_INSTALL
  - folder: third_party/fbgemm
    git_url: https://github.com/pytorch/fbgemm.git
    git_rev: dbc3157bf256f1339b3fa1fef2be89ac4078be0e
    patches:
      - patches_submodules/fbgemm/0001-remove-DESTINATION-lib-from-CMake-install-directives.patch     # [win]
  - folder: third_party/flash-attention
    git_url: https://github.com/Dao-AILab/flash-attention.git
    git_rev: 979702c87a8713a8e0a5e9fee122b90d2ef13be5
  - folder: third_party/flatbuffers
    git_url: https://github.com/google/flatbuffers.git
    git_rev: 01834de25e4bf3975a9a00e816292b1ad0fe184b
  - folder: third_party/fmt
    git_url: https://github.com/fmtlib/fmt.git
    git_rev: 123913715afeb8a437e6388b4473fcc4753e1c9a
  - folder: third_party/gemmlowp/gemmlowp
    git_url: https://github.com/google/gemmlowp.git
    git_rev: 3fb5c176c17c765a3492cd2f0321b0dab712f350
  - folder: third_party/gloo
    git_url: https://github.com/facebookincubator/gloo.git
    git_rev: 5354032ea08eadd7fc4456477f7f7c6308818509
  - folder: third_party/googletest
    git_url: https://github.com/google/googletest.git
    git_rev: b514bdc898e2951020cbdca1304b75f5950d1f59
  - folder: third_party/ideep
    git_url: https://github.com/intel/ideep.git
    git_rev: 719d8e6cd7f7a0e01b155657526d693acf97c2b3
  - folder: third_party/ittapi
    git_url: https://github.com/intel/ittapi.git
    git_rev: 5b8a7d7422611c3a0d799fb5fc5dd4abfae35b42
  - folder: third_party/kineto
    git_url: https://github.com/pytorch/kineto.git
    git_rev: a054a4be0db117c579a21747debf19c863631f26
  - folder: third_party/kleidiai
    git_url: https://github.com/ARM-software/kleidiai.git
    git_rev: ef685a13cfbe8d418aa2ed34350e21e4938358b6
  - folder: third_party/mimalloc
    git_url: https://github.com/microsoft/mimalloc.git
    git_rev: b66e3214d8a104669c2ec05ae91ebc26a8f5ab78
  - folder: third_party/nlohmann
    git_url: https://github.com/nlohmann/json.git
    git_rev: 87cda1d6646592ac5866dc703c8e1839046a6806
  - folder: third_party/onnx
    git_url: https://github.com/onnx/onnx.git
    git_rev: b8baa8446686496da4cc8fda09f2b6fe65c2a02c
  - folder: third_party/opentelemetry-cpp
    git_url: https://github.com/open-telemetry/opentelemetry-cpp.git
    git_rev: a799f4aed9c94b765dcdaabaeab7d5e7e2310878
  - folder: third_party/pocketfft
    git_url: https://github.com/mreineck/pocketfft.git
    git_rev: 9d3ab05a7fffbc71a492bc6a17be034e83e8f0fe
  # - folder: third_party/protobuf  # skipped due to BUILD_CUSTOM_PROTOBUF=OFF
  - folder: third_party/psimd
    git_url: https://github.com/Maratyszcza/psimd.git
    git_rev: 072586a71b55b7f8c584153d223e95687148a900
  - folder: third_party/pthreadpool
    git_url: https://github.com/Maratyszcza/pthreadpool.git
    git_rev: 4fe0e1e183925bf8cfa6aae24237e724a96479b8
  # - folder: third_party/pybind11  # skipped due to USE_SYSTEM_PYBIND11
  - folder: third_party/python-peachpy
    git_url: https://github.com/malfet/PeachPy.git
    git_rev: f45429b087dd7d5bc78bb40dc7cf06425c252d67
  # - folder: third_party/sleef  # skipped due to USE_SYSTEM_SLEEF
  - folder: third_party/tensorpipe
    git_url: https://github.com/pytorch/tensorpipe.git
    git_rev: 52791a2fd214b2a9dc5759d36725909c1daa7f2e
    patches:
      - patches_submodules/tensorpipe/0001-switch-away-from-find_package-CUDA.patch

build:
  number: {{ build }}
  # cuda 11.8 was dropped due to maintenance effort, see discussion in #177
  skip: true  # [cuda_compiler_version == "11.8"]
  # This logic allows two rc variants to be defined in the conda_build_config, but only one to actually be built.
  # We want to be able to define two variants in the cbc so we can assign different labels to each in the upload channel
  # (by zipping is_rc with channel_targets). This prevents rc builds being used unless specifically requested.
{% if "rc" in version %}
  skip: true  # [not is_rc]
{% else %}
  skip: true  # [is_rc]
{% endif %}
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_{{ blas_impl }}_h{{ PKG_HASH }}_{{ build }}  # [cuda_compiler_version != "None"]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ build }}                                                 # [cuda_compiler_version == "None"]
  detect_binary_files_with_prefix: false
  run_exports:
    - {{ pin_subpackage('libtorch', max_pin='x.x') }}
  ignore_run_exports_from:
    - python *                               # [megabuild]
    - numpy *                                # [megabuild]
    - cross-python_{{ target_platform }}     # [megabuild and build_platform != target_platform]
  ignore_run_exports:
    - python *                               # [megabuild]
    - numpy *                                # [megabuild]
    - libmagma_sparse

requirements:
  # Keep this list synchronized (except for python*, numpy*) in outputs
  # We use python to build libtorch as well because it is easier
  build:
    # When you change 3.12 here, change it in build.sh/bld.bat as well
    - python 3.12                            # [megabuild and build_platform != target_platform]
    - python                                 # [not megabuild and build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
    - numpy  *                               # [megabuild and build_platform != target_platform]
    - numpy                                  # [not megabuild and build_platform != target_platform]
    - {{ stdlib('c') }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                 # [cuda_compiler_version != "None"]
    - llvm-openmp               # [unix]
    - intel-openmp {{ mkl }}    # [win]
    - libuv                     # [win]
    - cmake <4
    - ninja
    # Keep libprotobuf here so that a compatibile version
    # of protobuf is installed between build and host
    - libprotobuf
    - protobuf
    - make      # [linux]
    - sccache   # [win]
    - grep      # [unix]
    - rsync     # [unix]
  host:
    # GPU requirements
    - cudnn                           # [cuda_compiler_version != "None"]
    - nccl                            # [cuda_compiler_version != "None" and linux]
    - magma                           # [cuda_compiler_version != "None"]
    - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
    - nvtx-c                          # [cuda_compiler_version != "None"]
    {% if cuda_compiler_version != "None" %}
    - cuda-driver-dev                 # [linux]
    - cuda-cudart-dev
    - cuda-cupti-dev
    - cuda-nvrtc-dev
    - cuda-nvtx-dev
    - cuda-nvml-dev
    - cuda-profiler-api
    - cusparselt
    - libcublas-dev
    - libcudss-dev
    - libcufile-dev  # [linux]
    - libcufft-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
    {% endif %}
    # other requirements
    - python 3.12  # [megabuild]
    - python       # [not megabuild]
    - numpy *      # [megabuild]
    - numpy        # [not megabuild]
    - pip
    # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/380
    - setuptools <76
    - pyyaml
    - requests
    - six
    - mkl-devel {{ mkl }}   # [blas_impl == "mkl"]
    - libcblas * *_mkl      # [blas_impl == "mkl"]
    - libblas               # [blas_impl != "mkl"]
    - libcblas              # [blas_impl != "mkl"]
    - liblapack             # [blas_impl != "mkl"]
    - llvm-openmp             # [unix]
    - intel-openmp {{ mkl }}  # [win]
    - libabseil
    - libprotobuf
    - sleef
    - libuv
    - pkg-config  # [unix]
    - typing_extensions
    - pybind11
    - eigen
    - zlib
  run:
    # GPU requirements without run_exports
    - {{ pin_compatible('cudnn') }}     # [cuda_compiler_version != "None"]
    - intel-openmp {{ mkl }}            # [win]
    - libblas * *{{ blas_impl }}        # [blas_impl == "mkl"]
  run_constrained:
    # These constraints ensure conflict between pytorch and
    # pytorch-cpu 1.1 which we built before conda-forge had GPU infrastructure
    # built into place.
    # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/65
    - pytorch-cpu {{ version }}    # [cuda_compiler_version == "None"]
    - pytorch-gpu <0.0a0           # [cuda_compiler_version == "None"]
    - pytorch-gpu {{ version }}    # [cuda_compiler_version != "None"]
    - pytorch-cpu <0.0a0           # [cuda_compiler_version != "None"]
    - pytorch {{ version }} cuda{{ cuda_compiler_version | replace('.', '') }}_{{ blas_impl }}_*_{{ build }}  # [cuda_compiler_version != "None"]
    - pytorch {{ version }} cpu_{{ blas_impl }}_*_{{ build }}                                                 # [cuda_compiler_version == "None"]
    # if using OpenBLAS, ensure that a version compatible with OpenMP is used
    # otherwise, we get the following warnings:
    # OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.
    - openblas * openmp_*          # [unix and blas_impl != "mkl"]

# these tests are for the libtorch output below, but due to
# a particularity of conda-build, that output is defined in
# the global build stage, including tests
test:
  requires:
    # cmake needs a compiler to run package detection, see
    # https://discourse.cmake.org/t/questions-about-find-package-cli-msvc/6194
    - {{ compiler('cxx') }}
    # for CMake config to find cuda & nvrtc
    - {{ compiler('cuda') }}    # [cuda_compiler_version != "None"]
    - cuda-nvrtc-dev            # [cuda_compiler_version != "None"]
    - nvtx-c                    # [cuda_compiler_version != "None"]
    - cmake <4
    - ninja
    - pkg-config
  files:
    - cmake_test/
  commands:
    # libraries; peculiar formatting to avoid linter false positives about selectors
    {% set torch_libs = [
        "c10", "shm", "torch", "torch_cpu", "torch_global_deps"
    ] + (cuda_compiler_version != "None" and target_platform.startswith("linux")) * [
        "torch_cuda_linalg"
    ] + (cuda_compiler_version != "None") * [
        "c10_cuda", "caffe2_nvrtc", "torch_cuda"
    ] + target_platform.startswith("win") * [
        "asmjit", "fbgemm"
    ]
    %}
    {% for each_lib in torch_libs %}
    - test -f $PREFIX/lib/lib{{ each_lib }}.so              # [linux]
    - test -f $PREFIX/lib/lib{{ each_lib }}.dylib           # [osx]
    - if not exist %LIBRARY_BIN%\{{ each_lib }}.dll exit 1  # [win]
    {% if each_lib != "torch_global_deps" %}
    - if not exist %LIBRARY_LIB%\{{ each_lib }}.lib exit 1  # [win]
    {% endif %}
    {% endfor %}

    # CMake files in share
    - test -f $PREFIX/share/cmake/Torch/TorchConfig.cmake                       # [linux]
    - if not exist %LIBRARY_PREFIX%\share\cmake\Torch\TorchConfig.cmake exit 1  # [win]

    # test integrity of CMake metadata
    - cd cmake_test
    - cmake -GNinja -DCMAKE_CXX_STANDARD=17 $CMAKE_ARGS .   # [unix]
    - cmake -GNinja -DCMAKE_CXX_STANDARD=17 %CMAKE_ARGS% .  # [win]

outputs:
  - name: libtorch
  - name: pytorch
    script: build.sh    # [unix]
    script: bld.bat     # [win]
    build:
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_{{ blas_impl }}_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ build }}  # [cuda_compiler_version != "None"]
      string: cpu_{{ blas_impl }}_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ build }}                                                 # [cuda_compiler_version == "None"]
      detect_binary_files_with_prefix: false
      run_exports:
        - {{ pin_subpackage('pytorch', max_pin='x.x') }}
        - {{ pin_subpackage('libtorch', max_pin='x.x') }}
      ignore_run_exports:
        - libmagma_sparse
    requirements:
      build:
        - python
        - cross-python_{{ target_platform }}     # [build_platform != target_platform]
        - numpy                                  # [build_platform != target_platform]
        - {{ stdlib('c') }}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                 # [cuda_compiler_version != "None"]
        - llvm-openmp             # [unix]
        - intel-openmp {{ mkl }}  # [win]
        - cmake <4
        - ninja
        # Keep libprotobuf here so that a compatibile version
        # of protobuf is installed between build and host
        - libprotobuf
        - protobuf
        - make      # [linux]
        - sccache   # [win]
      host:
        - {{ pin_subpackage('libtorch', exact=True) }}
        # GPU requirements
        - cudnn                           # [cuda_compiler_version != "None"]
        - nccl                            # [cuda_compiler_version != "None" and linux]
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - nvtx-c                          # [cuda_compiler_version != "None"]
        - magma                           # [cuda_compiler_version != "None"]
        {% if cuda_compiler_version != "None" %}
        - cuda-driver-dev                 # [linux]
        - cuda-cudart-dev
        - cuda-cupti-dev
        - cuda-nvrtc-dev
        - cuda-nvtx-dev
        - cuda-nvml-dev
        - cuda-profiler-api
        - cusparselt
        - libcublas-dev
        - libcudss-dev
        - libcufile-dev  # [linux]
        - libcufft-dev
        - libcurand-dev
        - libcusolver-dev
        - libcusparse-dev
        {% endif %}
        # other requirements
        - python
        - numpy
        - pip
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/380
        - setuptools <76
        - pyyaml
        - requests
        - six
        - mkl-devel {{ mkl }}   # [blas_impl == "mkl"]
        - libcblas * *_mkl      # [blas_impl == "mkl"]
        - libcblas              # [blas_impl != "mkl"]
        - liblapack             # [blas_impl != "mkl"]
        - llvm-openmp             # [unix]
        - intel-openmp {{ mkl }}  # [win]
        - libabseil
        - libprotobuf
        - pybind11
        - eigen
        - sleef
        - libuv
        - pkg-config  # [unix]
        - typing_extensions
        - zlib
      run:
        - {{ pin_subpackage('libtorch', exact=True) }}  # [megabuild]
        # for non-megabuild, allow libtorch from any python version
        - libtorch {{ version }}.* *_{{ build }}        # [not megabuild]
        - llvm-openmp                       # [unix]
        - intel-openmp {{ mkl }}            # [win]
        - libblas * *{{ blas_impl }}        # [blas_impl == "mkl"]
        - nomkl                             # [blas_impl != "mkl"]
        # GPU requirements without run_exports
        - {{ pin_compatible('cudnn') }}     # [cuda_compiler_version != "None"]
        - triton {{ triton }}               # [cuda_compiler_version != "None" and not win]
        # avoid that people without GPUs needlessly download ~0.5-1GB
        - __cuda                            # [cuda_compiler_version != "None"]
        - python
        # other requirements, see https://github.com/pytorch/pytorch/blame/main/requirements.txt
        - filelock
        - fsspec
        - jinja2
        - networkx
        - optree >=0.13.0
        - pybind11
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/380; also used at runtime, see
        # https://github.com/pytorch/pytorch/blob/v2.6.0/torch/utils/cpp_extension.py#L2166-2172
        - setuptools <76
        - sympy >=1.13.3
        - typing_extensions >=4.10.0
      run_constrained:
        # These constraints ensure conflict between pytorch and
        # pytorch-cpu 1.1 which we built before conda-forge had GPU infrastructure
        # built into place.
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/65
        - pytorch-cpu {{ version }}    # [cuda_compiler_version == "None"]
        - pytorch-gpu <0.0a0           # [cuda_compiler_version == "None"]
        - pytorch-gpu {{ version }}    # [cuda_compiler_version != "None"]
        - pytorch-cpu <0.0a0           # [cuda_compiler_version != "None"]

    test:
      requires:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        # for torch.compile tests
        - {{ compiler('cuda') }}       # [cuda_compiler_version != "None"]
        - ninja
        - boto3
        - hypothesis
        - pytest
        - tabulate
        - pydot
        - pip
        - expecttest
        - xmlrunner
        # Required by run_test.py
        - pytest-flakefinder
        - pytest-rerunfailures
        - pytest-xdist
        # danpetry/TF: Pytorch includes their own edited version of pytest-shard and adding
        # it into the test deps as well results in the --shard-id option being added twice.
        # https://github.com/pytorch/pytorch/blob/main/test/pytest_shard_custom.py
        # - pytest-shard
        # for cmake_test
        - cmake <4
        - cuda-nvrtc-dev            # [cuda_compiler_version != "None"]
        - nvtx-c                    # [cuda_compiler_version != "None"]
        - pybind11
      imports:
        - torch
        - torch._C
      files:
        - cmake_test/
      source_files:
        # Only include the source_files if we are actually going to run the tests.
        - test
        # tools/ is needed to optimise test run
        # as of pytorch=2.0.0, there is a bug when trying to run tests without the tools
        - tools
      commands:
        # Run pip check so as to ensure that all pytorch packages are installed
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
        - pip check
        - python -c "import torch; print(torch.__version__)"
        - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [x86 and cuda_compiler_version == "None"]
        - python -c "import torch; torch.tensor(1).to('cpu').numpy(); print('numpy support enabled!!!')"
        # We have had issues with openmp .dylibs being doubly loaded in certain cases. These two tests catch the (observed) issue
        - python -c "import torch; import numpy"
        - python -c "import numpy; import torch"
        # distributed support is enabled by default on linux; for mac, we enable it manually in build.sh
        - python -c "import torch; assert torch.distributed.is_available()"         # [linux or osx]
        - python -c "import torch; assert torch.backends.cuda.is_built()"           # [cuda_compiler_version != "None"]
        - python -c "import torch; assert torch.backends.cudnn.is_available()"      # [cuda_compiler_version != "None"]
        - python -c "import torch; assert torch.backends.cudnn.enabled"             # [cuda_compiler_version != "None"]
        - python -c "import torch; assert torch.version.cuda is not None"           # [cuda_compiler_version != "None"]
        # At conda-forge, we target versions of OSX that are too old for MPS support
        # But if users install a newer version of OSX, they will have MPS support
        # https://github.com/conda-forge/pytorch-cpu-feedstock/pull/123#issuecomment-1186355073
        # - python -c "import torch; assert torch.backends.mps.is_available()" # [osx]

        # python-version-specific library (default location in SP_DIR symlinks back to this)
        - test -f $PREFIX/lib/libtorch_python${SHLIB_EXT}           # [unix]
        - if not exist %LIBRARY_BIN%\torch_python.dll exit 1        # [win]
        - if not exist %SP_DIR%\torch\lib\torch_python.lib exit 1   # [win]
        - if not exist %SP_DIR%\torch\lib\_C.lib exit 1             # [win]

        # a reasonably safe subset of tests that should run under 15 minutes
        {% set tests = " ".join([
            "test/test_autograd.py",
            "test/test_autograd_fallback.py",
            "test/test_custom_ops.py",
            "test/test_linalg.py",
            "test/test_mkldnn.py",
            "test/test_modules.py",
            "test/test_nn.py",
            "test/test_torch.py",
            "test/test_xnnpack_integration.py",
        ]) %}
        # tests torch.compile; avoid on aarch because it adds >4h in test runtime in emulation;
        # they add a lot of runtime (15->60min on windows), so run them for only one python version
        {% set tests = tests ~ " test/inductor/test_torchinductor.py" %}    # [py==312 and not (aarch64 or osx)]

        {% set skips = "(TestTorch and test_print)" %}
        # minor tolerance violations
        {% set skips = skips ~ " or test_1_sized_with_0_strided_cpu_float32" %}         # [osx]
        {% set skips = skips ~ " or test_batchnorm_nhwc_cpu" %}                         # [unix]
        # timeouts and failures on aarch, see https://github.com/conda-forge/pytorch-cpu-feedstock/pull/298#issuecomment-2555888508
        {% set skips = skips ~ " or test_pynode_destruction_deadlock" %}                # [aarch64]
        {% set skips = skips ~ " or (TestLinalgCPU and test_cholesky_cpu_float32)" %}   # [aarch64]
        {% set skips = skips ~ " or (TestLinalgCPU and test_pca_lowrank_cpu)" %}        # [aarch64]
        {% set skips = skips ~ " or (TestLinalgCPU and test_svd_lowrank_cpu)" %}        # [aarch64]
        {% set skips = skips ~ " or (TestMkldnnCPU and test_lstm_cpu)" %}               # [aarch64]
        # doesn't crash, but gets different result on aarch + CUDA
        {% set skips = skips ~ " or illcondition_matrix_input_should_not_crash_cpu" %}  # [aarch64 and cuda_compiler_version != "None"]
        # may crash spuriously
        {% set skips = skips ~ " or (TestAutograd and test_profiler_seq_nr)" %}
        {% set skips = skips ~ " or (TestAutograd and test_profiler_propagation)" %}
        # tests that fail due to resource clean-up issues (non-unique temporary libraries), see
        # https://github.com/conda-forge/pytorch-cpu-feedstock/pull/318#issuecomment-2620080859
        {% set skips = skips ~ " or test_mutable_custom_op_fixed_layout" %}
        # trivial accuracy problems
        {% set skips = skips ~ " or test_BCELoss_weights_no_reduce_cuda" %}             # [unix and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or test_ctc_loss_cudnn_tensor_cuda " %}                # [unix and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or (TestTorch and test_index_add_correctness)" %}      # [unix and cuda_compiler_version != "None"]
        # These tests require higher-resource or more recent GPUs than the CI provides
        {% set skips = skips ~ " or test_sdpa_inference_mode_aot_compile" %}            # [linux and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or (TestNN and test_grid_sample)" %}                   # [linux and cuda_compiler_version != "None"]
        # don't mess with tests that rely on GPU failure handling
        {% set skips = skips ~ " or test_cublas_config_nondeterministic_alert_cuda" %}  # [linux and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or test_cross_entropy_loss_2d_out_of_bounds_class" %}  # [linux and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or test_indirect_device_assert" %}                     # [linux and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or test_reentrant_parent_error_on_cpu_cuda" %}         # [linux and cuda_compiler_version != "None"]
        # test that fails to find temporary resource
        {% set skips = skips ~ " or (GPUTests and test_scatter_reduce2)" %}             # [linux and cuda_compiler_version != "None"]
        # ROCM test whose skip doesn't trigger
        {% set skips = skips ~ " or test_ck_blas_library_cpu" %}                        # [linux and cuda_compiler_version != "None"]
        # problem with finding output of `torch.cuda.tunable.write_file()`
        {% set skips = skips ~ " or test_matmul_offline_tunableop_cuda_float16" %}      # [linux and cuda_compiler_version != "None"]
        # catastropic accuracy failure in convolution
        {% set skips = skips ~ " or test_Conv3d_1x1x1_no_bias_cuda" %}                  # [linux and cuda_compiler_version != "None"]
        # skip some very long-running groups of tests (~30 minutes total)
        {% set skips = skips ~ " or (test_gradgrad_nn_Transformer and _cuda_)" %}       # [linux and cuda_compiler_version != "None"]
        {% set skips = skips ~ " or test_avg_pool3d_backward2" %}                       # [linux and cuda_compiler_version != "None"]
        # MKL problems
        {% set skips = skips ~ " or (TestLinalgCPU and test_inverse_errors_large_cpu)" %}           # [linux and blas_impl == "mkl" and cuda_compiler_version != "None"]
        # non-MKL problems
        {% set skips = skips ~ " or test_gather_scatter_cpu or test_index_put2_cpu " %}             # [linux and blas_impl != "mkl" and cuda_compiler_version != "None"]
        # these tests are failing with low -n values
        {% set skips = skips ~ " or test_base_does_not_require_grad_mode_nothing" %}
        {% set skips = skips ~ " or test_base_does_not_require_grad_mode_warn" %}
        {% set skips = skips ~ " or test_composite_registered_to_cpu_mode_nothing" %}
        # these tests are failing on windows
        {% set skips = skips ~ " or (TestNN and test_Conv1d_dilated)" %}                 # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv1d_pad_same_dilated)" %}        # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv2d_pad_same_dilated)" %}        # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv2d_padding)" %}                 # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv2d_strided)" %}                 # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv3d_dilated)" %}                 # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv3d_dilated_strided)" %}         # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv3d_pad_same_dilated)" %}        # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv3d_stride)" %}                  # [win]
        {% set skips = skips ~ " or (TestNN and test_Conv3d_stride_padding)" %}          # [win]
        # flaky test, fragile to GC behavior
        {% set skips = skips ~ " or (TestTorch and test_tensor_cycle_via_slots)" %}

        # the whole test suite takes forever, but we should get a good enough coverage
        # for potential packaging problems by running a fixed subset
        - export OMP_NUM_THREADS=4  # [unix]
        # reduced paralellism to avoid OOM for CUDA builds
        {% set jobs = "-n 2" %}
        {% set jobs = "-n 1" %}     # [linux64 and cuda_compiler_version != "None"]
        # test only one python version on aarch because emulation is super-slow;
        # disable hypothesis because it randomly yields health check errors
        - pytest {{ jobs }} {{ tests }} -k "not ({{ skips }})" -m "not hypothesis" --durations=50 --disable-warnings    # [not aarch64 or py==312]

        # regression test for https://github.com/conda-forge/pytorch-cpu-feedstock/issues/329, where we picked up
        # duplicate `.pyc` files due to newest py-ver (3.13) in the build environment not matching the one in host;
        # obviously this test can only be done for other python versions.
        - test ! -f $SP_DIR/functorch/__pycache__/__init__.cpython-313.pyc          # [py!=313 and unix]
        - if exist %SP_DIR%\functorch\__pycache__\__init__.cpython-313.pyc exit 1   # [py!=313 and win]

        # test integrity of CMake metadata and ensure that THPLayoutType is visible as a symbol from libtorch_python
        - cd cmake_test
        - cmake -GNinja -DCMAKE_CXX_STANDARD=17 -DWITH_TORCH_PYTHON=ON $CMAKE_ARGS .   # [unix]
        - cmake -GNinja -DCMAKE_CXX_STANDARD=17 -DWITH_TORCH_PYTHON=ON %CMAKE_ARGS% .  # [win]
        - cmake --build .                   # [unix]
        - cmake --build . --config Release  # [win]

  # 2021/08/01, hmaarrfk
  # While this seems like a roundabout way of defining the package name
  # It helps the linter avoid errors on a package not having tests.
  {% set pytorch_cpu_gpu = "pytorch-cpu" %}   # [cuda_compiler_version == "None"]
  {% set pytorch_cpu_gpu = "pytorch-gpu" %}   # [cuda_compiler_version != "None"]
  - name: {{ pytorch_cpu_gpu }}
    build:
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_{{ blas_impl }}_h{{ PKG_HASH }}_{{ build }}    # [megabuild and cuda_compiler_version != "None"]
      string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ build }}                                                   # [megabuild and cuda_compiler_version == "None"]
      string: cpu_{{ blas_impl }}_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ build }}                                  # [not megabuild]
      detect_binary_files_with_prefix: false
      # weigh down cpu implementation and give cuda preference
      track_features:
        - pytorch-cpu                                      # [cuda_compiler_version == "None"]
    requirements:
      run:
        - pytorch {{ version }} cuda*_{{ blas_impl }}*{{ build }}   # [megabuild and cuda_compiler_version != "None"]
        - pytorch {{ version }} cpu_{{ blas_impl }}*{{ build }}     # [megabuild and cuda_compiler_version == "None"]
        - {{ pin_subpackage("pytorch", exact=True) }}               # [not megabuild]
    test:
      imports:
        - torch

about:
  home: https://pytorch.org/
  dev_url: https://github.com/pytorch/pytorch
  license: BSD-3-Clause
  license_family: BSD
  license_file:
    - LICENSE
    - NOTICE
    - third_party/CMake/Copyright.txt
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
  description: |
    PyTorch is a Python package that provides two high-level features:
      - Tensor computation (like NumPy) with strong GPU acceleration
      - Deep neural networks built on a tape-based autograd system
    You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.
  doc_url: https://pytorch.org/docs/

extra:
  recipe-maintainers:
    - baszalmstra
    - benjaminrwilson
    - beckermr
    - h-vetinari
    - hmaarrfk
    - jeongseok-meta
    - mgorny
    - sodre
    - Tobias-Fischer
  feedstock-name: pytorch-cpu
